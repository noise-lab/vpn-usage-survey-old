\subsection{Method}

To answer our first two research questions about  what uses mental models of
virtual private networks are and how they use them,  we conducted
qualitative interviews and a large scale survey. 


\subsubsection{Interviews and Surveys} 

\paragraph{Interviews}
Before participating in a semi-structured interview, participants were asked
to fill out a consent form and a short survey, where we collected data about
their demographic information and their general online habits and behavior.
Our interview guide was structured first to get a better understanding of
participants' knowledge and background and participants' general privacy and
security awareness. Thus, we asked
participants to describe how a VPN works. For example, we asked how they learned about VPNs and what
their first experience using a VPN was. Next, we asked how participants choose
to use a particular VPN and why and how they use a VPN at all. For example, we asked
how participants felt when using a VPN and whether they use a paid or free
version of VPN.  
 
\paragraph{Survey}
Based on the interview data and analysis, we then designed a larger-scale survey.  The survey was split into 8 categories: Pre-screening, Demographics, Privacy and
Security Awareness, Privacy and Security Practices, VPN Perception, VPN
Preferences, VPN Usage, and VPN Issues and Improvements. The pre-screening questions
filtered out respondents who did not consent to the survey, were under 18, or
had never used a VPN. We collected academic
majors and other basic demographic information such as age, gender, and course of study. Similar to the interviews, we collected background information in the Privacy and Security Awareness questions about respondents' perception
and concern about online data collection, including the nature of data
collected, who is collecting data, and why they are collecting data. We also asked about respondents' usage patterns
of different tools and tactics to combat online risks, as well as how they
sourced them.

As in the interviews, we asked about students’ knowledge and usage
patterns of different VPN types, including specific VPNs they had used. We
also asked questions on respondents’ motivations in selecting and using VPNs. We also asked specifically about student
perceptions of data collection by VPNs as well as feelings while using VPNs.
Finally, we asked about issues that users faced while using VPNs.  In designing the survey, we generally avoided
open-ended questions to prevent user fatigue and reduce the complexity of data
analysis; as a result, we asked only 3 open-ended questions. We also avoided
double-barreled questions, negative questions, and biased wording
\cite{lazar_28}. We included two attention check questions that required a
certain response to ensure respondents were answering mindfully. 

\subsubsection{Recruitment and Participants} 

\paragraph{Recruitment}
We recruited 32 interview participants through emailing listservs of our
institution's survey center, Twitter and we also posted an advertisement on
our research lab' website. We filtered for students who had used a VPN before;
were students currently enrolled in a US university undergraduate or graduate
program.  We aim to recruit a variety of international and domestic students
living in the United States.  We concluded that such diverse group would
expand our knowledge and understanding on how and why participants use VPN.
Interviews were conducted in Summer and Fall 2018. Participants were
compensated with a \$20 Amazon gift card.  Interviewees could choose between
meeting on Princeton University campus and remotely, through Skype. We
conducted 23 interviews via Skype and 9 were conducted on our university
campus.  Four interviewees did not give consent to recording so detailed notes
were taken during these interviews. All other interviews were audio-taped.

For the survey, we recruited undergraduate and graduate students from our
institution to take the survey on Qualtrics. We sent email invitations to a
random sample of 2,748 people in this audience via an institutional survey
research center. We included filtering questions for VPN users who were over
the age of 18, who had used a VPN before, and who had not participated in the
interview study. Our final sample of 350 valid and completed responses is
large compared to university's population (4.3\%). We launched and conducted
the survey between February 2019-March 2019. We collected 452 responses, of
which 392 were generated by respondents who fit our criteria. Of these 392
responses, 356 were fully completed. Of the 356 complete responses, 350 passed
our attention checks and are considered valid responses for the purposes of
analysis. Survey compensation was provided through a lottery drawing of valid,
completed responses for one of two \$250 Amazon gift cards.

\paragraph{Participants}
We conducted 32 interviews with 20
international students and 12 domestic students. Table~\ref{table:1} shows demographic data of our intreviewees, who were mostly 18-24 years old, undergraduate students (81\%).

\begin{table*}[h!]
\centering
\begin{tabular}{l r r |l r r|l r r|l r r} 
 \hline
 Age & \# & \% & Gender & \# & \% & Origin & \# & \% & Educational status & \# & \% \\
\hline
18 to 24 & 26 & 81\% & Female & 17 & 53\% & United States & 12 & 38\% & Postdoctoral Researchers & 4 & 13\% \\
25 to 34 & 5 & 16\% & Male & 14 & 44\% & International & 20 & 63\% & Graduate students & 2 & 6\% \\
35 to 44 & 1 & 3\% & Other & 1 & 3\% & & & & Undergraduate students & 26 & 81 \% \\
 \hline
\end{tabular}
\caption{The distribution over age, gender, origin and education status for 32 interview participants, at the time of collecting the data. Our 20 International participants came from 17 different countries.}
\label{table:1}
\end{table*}

\begin{table*}[h!]
\centering
\begin{tabular}{l r r |l r r|l r r|l r r} 
 \hline
 Age & \# & \% & Gender & \# & \% & Origin & \# & \% & Educational status & \# & \% \\
\hline
18 to 25 & 275 & 79\% & Female & 178 & 51\% & United States & 258 & 74\% & Graduate Students & 123 & 35\% \\
26 to 35 & 74 & 21\% & Male & 172 & 49\% & International & 92 & 26\% & Undergraduate Students & 227 & 65\% \\
36+ & 1 & 0\% & & & & & & & & & \\
 \hline
\end{tabular}
\caption{The distribution over age, gender, origin and education status for 350 survey participants, at the time of collecting the data. Our 92 International participants came from 32 different countries.}
\label{table:2}
\end{table*}

Table~\ref{table:2} shows detailed demographic data of the respondents. As expected, the
majority of them were age 25 and under (79\%, 275/350). The vast majority of
the respondents were American nationals (74\%, 258/350); the countries with
the next-highest representation were China (6\%, 22/350) and Canada (4\%,
13/350). Most of the respondents were enrolled in an undergraduate
program (63\%, 219/350), with 28\% (97/350) of them enrolled in a doctorate
program. Of the undergraduate respondents, 34\% (74/219) were fourth-year
students. The most popular majors among respondents were computer science
(13\%, 46/350), economics (9\%, 33/350), public policy (7\%, 26/350), and
molecular biology (7\%, 25/350). Notably, computer science majors were not
overrepresented in our sample.




\subsubsection{Data Processing and Analysis} 

\paragraph{Interviews} We first transcribed all recorded interviews and developed extensive codebook
to apply to the interview transcripts and field notes. We used Dedoose
software for all interviews analysis. The codebook was first based on set of interview
questions but we refined the codebook as we analyzed more data. One of the research team coded all of the interview transcripts and a second member of the team reviewed the codes. We had 1906
codes in total, including 45 parent codes, for example \textit{Guidelines when
choosing VPN} and \textit{Possible improvments}; and 1861 child codes, like
\textit{Good reputation of VPN} and \textit{More user-friendly} respectively. Once the transcripts were all coded, the researcher involved in coding wrote up summaries of the coded transcripts and themes arising from this initial analysis phase. The rest of the research team reviewed the summaries and held regular research meetings to decide on the final themes arising from the interviews.

\paragraph{Surveys} We used Qualtrics and R as software for the survey data analysis. We first analyzed the
response data using tools built-in with Qualtrics. We limited our analysis to
the 350 valid and complete responses. First, we performed descriptive analysis on all the survey questions. All survey questions were required for respondents, but individual questions
were shown to respondents only when applicable. As such, questions that have
fewer than 350 data points contain responses from every applicable respondent;
a lack of response does not indicate a respondent's choice to abstain. In
presenting our results, we discuss percentages in terms of the number of
people who were shown the question. However, the figures presented in this
section will display percentages in terms of the total number of respondents
(350), and include "N/A" values for those who were not shown the question.

Next, we searched for statistically
significant correlations by finding pairwise correlations for each variable.
The raw response data was unsound for this purpose as it contained 605
variables, including separate variables for each checkbox in
multiple-selection questions. Attempting to analyze a dataset this large
through Qualtrics software resulted in erratic behavior, and would not produce
meaningful results without first cleaning the data using methods not available
in Qualtrics. We cleaned the response dataset using R, and reduced the number
of variables by consolidating them into fewer categories when possible. For
example, our question on what tools and tactics respondents used to protect
themselves online had 20 variables -- one for each tool. We consolidated these
variables into a single "protection effort score" by assigning an effort score
to each tool, based on how difficult it was to execute, and summing these
scores for each response. This allowed us to both decrease the number of
observed variables to 124, and also increase the number of observations for
each variable. Appendix B contains more information on how each variable was
consolidated. \ax{what to include in appendices? might need to pare this down depending on that}

We chose to analyze pairwise correlations instead of using more in-depth
methods of data analysis for two reasons. First, this study intends to produce
a broad overview of our topic; as a result, the sheer number of observed
variables made it impractical to deliver detailed relationships between each
combination of variables. Second, given that our survey could not be
exhaustive, there are doubtless many unobserved variables that could
significantly affect the accuracy of any detailed analyses. Given the nature
of this study, we decided that the goal of this part of our analysis would be
to quantify the degrees to which our observed variables were correlated using
R and p values. Because our analysis involved the presence of both unobserved variables and
the unpredictability of human behavior, we expected our correlation analysis
to produce low R values. We will only be including correlations that are
notable and statistically significant (p < .05). Unless otherwise noted, R
values range from .1 to .3. \ax{corr -- remove if no longer using}

\subsubsection{Limitations}

We chose to limit our data
analysis to correlations due to our constraints. Future studies could explore
a subset of our variables in-depth, and potentially account for confounding
factors to establish stronger relationships. In addition, although we
speculate on relationships between variables, our correlations do not have
definitive directions of causality; further inquiry is needed to confirm any
assumptions on one variable influencing another. In addition, the methods we
used to consolidate our variables are just one interpretation of how to
categorize our choices. Other researchers may find different combinations that
produce new results.

Our survey also had some inherent drawbacks. Recall bias is difficult to avoid
in any survey \cite{lazar_28}. In addition, while our sample size of 350 was sufficiently
large, we analyzed intersections of variables that had different N values,
some of which could be small enough to introduce sampling error \cite{lazar_28}. Our
survey was also not completely anonymous as it required participants who
wished to enter our compensation drawing to submit an email. This could
introduce error in the respondents' levels of honesty. Future researchers
could build on our results using alternative methods.

Finally, our survey pool was limited to one institution's students and may not be representative of all college students in the US. Future researchers could replicate our study
with a more general college student population, or with other populations of
interest.
