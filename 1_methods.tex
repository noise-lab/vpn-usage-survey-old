To answer our first two research questions about  what uses mental models of virtual private networks are and how they use them,  we first conducted qualitative
interviews and a large scale survey. 

\subsection{Recruitment} We recruited interview participants through
emailing listservs of our institution's survey center, Twitter and we also
posted an advertisement on  our research lab' website. We filtered for students who had used a VPN before; were students currently enrolled in a US university undergraduate or graduate program.  we aim to recruit a variety of international and domestic students living in the United States.
We concluded that such diverse group would expand our knowledge and
understanding on how and why participants use VPN. All interviews were audio-taped and participants were compensated with a $20$.

For the survey, we recruited undergraduate and
graduate students from our institution. We sent email invitations to a random sample of 2,748 people in this
audience, and included a note in the email to filter for VPN users who are
over the age of 18. We collected 452 responses, of which 392 were generated by
respondents who fit our criteria. Of these 392 responses, 356 were fully
completed. Of the 356 complete responses, 350 passed our attention checks and
are considered valid responses for the purposes of analysis.

\subsection{Interviews} Before participating in an interview, participants were asked to fill
out a consent form and a short survey,
where we collected data about their demographic information and their general
online habits and behavior. Our interviews were semi-structured, so we asked
all participants the same set of questions and asked additional follow-up
questions whenever it was needed. To get a better
understanding of participants' knowledge and background, we  first asked  about
 participants' general privacy and security awareness. Next, we asked how  participants choose  to use a particular VPN and why they use them. MORE DETAIL ON WHAT QUESTIONS/EXAMPLE QS
 
 \subsection{Survey Methods} 
 \subsubsection{Design}

We chose to design a survey for our study as surveys are effective for
collecting a large number of responses to describe a diverse, concentrated
student population with low cost-per-response and high accessibility [28]. Our
audience was Princeton students who are 18 and over and had used VPN before.
We recruited participants through the Princeton Survey Research Center, which
has access to a directory of all undergraduate and graduate students. We
randomly sampled 2,748 students in three waves, and filtered these students
through our invitation email and through pre-screening survey questions. Our
final sample of 350 valid and completed responses is large compared to
Princeton University's population (4.3\%), and we can assume that it is a
sufficiently large proportion (>5\%) of VPN-using-students at Princeton to
draw meaningful conclusions with [28]. We utilized the Survey Research
Center's capabilities over launching a public survey in order to minimize
self-selection bias.

We built the survey in the Qualtrics software, as a license was provided by
the Survey Research Center. The survey was previewed and tested by members of
the Human-Computer Interaction lab group to refine and fix bugs from 2/6/19 to
2/25/19. The survey was launched to the first wave of respondents on 2/26/19,
the second wave of respondents on 3/4/19, and the third wave of respondents on
3/12/19; respondents were provided with reminder emails 3 days after being
invited. We closed the survey to further responses on 3/29/19.

Survey compensation was provided through a lottery drawing of valid, completed
responses for one of two \$250 Amazon gift cards.

\subsubsection{Content}

We split the survey into 8 blocks: Pre-screening, Demographics, Privacy and
Security Awareness, Privacy and Security Practices, VPN Perception, VPN
Preferences, VPN Usage, and VPN Issues and Improvements. Note that these
survey blocks were created solely for internal organizational purposes, and
they cannot be viewed by survey respondents. As such, the block titles do not
correspond 1-to-1 with their contents, or with our Results categories. The
original survey can be found in Appendix B.  The pre-screening questions
filtered out respondents who did not consent to the survey, were under 18, or
had never used VPN. The Demographics block included a question on academic
major, as we aimed to study online behaviors related to students in different
academic fields.

The Privacy and Security Awareness block asked about respondents' perception
and concern about online data collection, including the nature of data
collected, who is collecting data, and why they are collecting data. The
Privacy and Security Practices block asked about respondents' usage patterns
of different tools and tactics to combat online risks, as well as how they
sourced them.

The VPN Perception and VPN Usage blocks aimed to gain insights into students’
knowledge and usage patterns of different VPN types, including specific VPNs
they had used. We also asked questions on respondents’ motivations in
selecting and using VPNs, as well as how they sourced VPNs. The VPN Utility
block centered around the user-provider relationship, and asked questions
about perception of data collection by VPNs as well as vulnerability while
using VPNs. The VPN Issues and Improvements block asked about issues that
users faced while using VPNs, and also included open-ended questions about
what respondents liked and disliked about their VPN services.

In designing the survey, we generally avoided open-ended questions to prevent
user fatigue and reduce the complexity of data analysis; as a result, we asked
only 3 open-ended questions. We also avoided double-barreled questions,
negative questions, and biased wording [28]. We included two attention check
questions that required a certain response, and we discarded responses that
did not pass both (6/356).


\subsection{Analysis} We first transcribed all recorded interviews and
developed extensive codebook to apply to the interview transcripts and field notes. ADD MORE HERE ABOUT CODING PROCESS e.g. type of coding process, how many coders. The
codebook was first based on set of interview questions but we refined the codebook as we analyzed more data. We had x parent codes in total,
for example xxx and xxx; and x child codes, like yyy and yyy.

We used Qualtrics and R as software for the survey data analysis. We first analyzed the
response data using tools built-in with Qualtrics. We limited our analysis to
the 350 valid and complete responses.  Next, we searched for statistically
significant correlations by finding pairwise correlations for each variable.
The raw response data was unsound for this purpose as it contained 605
variables, including separate variables for each checkbox in
multiple-selection questions. Attempting to analyze a dataset this large
through Qualtrics software resulted in erratic behavior, and would not produce
meaningful results without first cleaning the data using methods not available
in Qualtrics. We cleaned the response dataset using R, and reduced the number
of variables by consolidating them into fewer categories when possible. For
example, our question on what tools and tactics respondents used to protect
themselves online had 20 variables -- one for each tool. We consolidated these
variables into a single "protection effort score" by assigning an effort score
to each tool, based on how difficult it was to execute, and summing these
scores for each response. This allowed us to both decrease the number of
observed variables to 124, and also increase the number of observations for
each variable. Appendix B contains more information on how each variable was
consolidated.

We chose to analyze pairwise correlations instead of using more in-depth
methods of data analysis for two reasons. First, this study intends to produce
a broad overview of our topic; as a result, the sheer number of observed
variables made it impractical to deliver detailed relationships between each
combination of variables. Second, given that our survey could not be
exhaustive, there are doubtless many unobserved variables that could
significantly affect the accuracy of any detailed analyses. Given the nature
of this study, we decided that the goal of this part of our analysis would be
to quantify the degrees to which our observed variables were correlated using
R and p values.

\subsection{Participants} In total, we conducted 32 interviews with 20
international students and 12 domestic students. Four interviewees did not give consent to
recording so detailed notes were taken during these interviews. Interviewees
could choose between meeting on Princeton University campus and remotely,
through Skype. We conducted 23 interviews via Skype and 9 were conducted
 on our university campus. 

TODO: table?


\begin{table*}[h!]
\centering
\begin{tabular}{l r r |l r r|l r r|l r r} 
 \hline
 Age & \# & \% & Gender & \# & \% & Origin & \# & \% & Educational status & \# & \% \\
\hline
18 to 24 & 26 & 81\% & Female & 17 & 53\% & United States & 12 & 38\% & Postdoctoral Researchers & 4 & 13\% \\
25 to 34 & 5 & 16\% & Male & 14 & 44\% & International & 20 & 63\% & Graduate students & 2 & 6\% \\
35 to 44 & 1 & 3\% & Other & 1 & 3\% & & & & Undergraduate students & 26 & 81 \% \\
 \hline
\end{tabular}
\caption{The distribution over age, gender, origin and education status for 32 interview participants, at the time of collecting the data. Our 20 International participants came from 17 different countries.}
\label{table:1}
\end{table*}


Figure 2 shows detailed demographic data of the respondents. As expected, the
majority of them were age 25 and under (79\%, 275/350). The vast majority of
the respondents were American nationals (74\%, 258/350); the countries with
the next-highest representation were China (6\%, 22/350) and Canada (4\%,
13/350). Most of the respondents were enrolled in Princeton's undergraduate
program (63\%, 219/350), with 28\% (97/350) of them enrolled in a doctorate
program. Of the undergraduate respondents, 34\% (74/219) were fourth-year
students. The most popular majors among respondents were computer science
(13\%, 46/350), economics (9\%, 33/350), public policy (7\%, 26/350), and
molecular biology (7\%, 25/350). Notably, computer science majors were not
overrepresented in our sample.

Almost all respondents (99\%, 348/350) believed that some data was collected
about them when they used the Internet. The vast majority of these respondents
(N=348) believed that companies (93\%, 323/348), websites (93\%, 322/348).
their government (83\%, 290/348), and Internet Service Provider (81\%,
282/348) were collecting their data; in contrast, only 2\% (8/348) of them
believed that friends and family were collecting data on them. Almost all of
these respondents believed that their data was collected for advertising and
other financial motives (99\%, 343/348); a smaller majority believed that
their data was collected for political motives, such as influencing political
leanings (72\%, 252/348). Almost all of these respondents believed that their
online activities (97\%, 339/348), interests and preferences (96\%, 335/348),
and location (96\%, 333/348) were collected. Smaller majorities believed that
demographic information (85\%, 293/348) and device type (81\%, 283/348) were
collected. Significant minorities of respondents believed that more sensitive
data including private messages (41\%, 141/348), keystrokes (31\%, 109/348),
and recordings (31\%, 109/348) were captured. On a five-point Likert scale,
participants were on average "somewhat concerned" about this data collection
(mean 2.94, median 3).

Survey respondents used a wide variety of tools and tactics outside of VPN.
Among the most popular were ad blockers (80\%, 280/350), using two-factor
authentication (75\%, 263/350), avoiding spam email (70\%, 246/350), and using
private browsing mode (63\%, 222/350). Fewer respondents utilized high-effort
tactics such as changing passwords frequently (15\%, 52/350), using password
managers (17\%, 60/350), or avoiding social media accounts (27\%, 95/350).
More obscure online tools were also less popular among respondents, with only
12\% (43/350) using tracker blockers such as Ghostery and 9\% (33/350) using
Tor. Most respondents (57\%, 198/350) used at least some of these tools most
of the time when they go online, on both laptops (98\%, 343/350) and phones
(77\%, 271/350). However, the overall effort that respondents put into
protecting themselves online was low. Out of a maximum score of 35, the median
"protection effort score" of respondents was only 12 (see section 3.5 for how
this statistic was derived). Participants largely heard about these tools and
tactics online (73\%, 256/350) and from friends and family (71\%, 248/350),
and most (64\%, 225/350) started using these tools and tactics 3 or more years
ago.

\subsection{Limitations}

This study is intended to provide an overview of college students' usage and
perceptions of VPNs; as discussed in section 3.5, we chose to limit our data
analysis to correlations due to our constraints. Future studies could explore
a subset of our variables in-depth, and potentially account for confounding
factors to establish stronger relationships. In addition, although we
speculate on relationships between variables, our correlations do not have
definitive directions of causality; further inquiry is needed to confirm any
assumptions on one variable influencing another. In addition, the methods we
used to consolidate our variables are just one interpretation of how to
categorize our choices. Other researchers may find different combinations that
produce new results.

Our survey also had some inherent drawbacks. Recall bias is difficult to avoid
in any survey [28]. In addition, while our sample size of 350 was sufficiently
large, we analyzed intersections of variables that had different N values,
some of which could be small enough to introduce sampling error [28]. Our
survey was also not completely anonymous as it required participants who
wished to enter our compensation drawing to submit an email. This could
introduce error in the respondents' levels of honesty. Future researchers
could build on our results using alternative methods.

Finally, we limited our survey pool to Princeton students due to ease of
access, and used our respondents as a representative sample for college
students in the US. However, this introduces sample bias as Princeton students
could display specific traits. Future researchers could replicate our study
with a more general college student population, or with other populations of
interest.
